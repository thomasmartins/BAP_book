{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40764979-ea93-4966-a84b-928de3d8da90",
   "metadata": {},
   "source": [
    "# Bayesian state space approach for nonsynchronous trading\n",
    "\n",
    "Previously we have mentioned the problem of nonsynchronous trading and some of its aspects, such as the bid-ask bounce. When we have a long time series with daily frequency, this does not have much of an impact, however in higher frequencies, e.g., minute by minute, this can be an issue.\n",
    "\n",
    "We will first look at a 1-minute frequency series of the price (and also returns) of a stock with lower trading volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f33034d-fe75-4a43-9015-fbcd54e997ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (pytensor.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as st\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns\n",
    "import yfinance as yf\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e7ad48e-80b8-4d24-a73a-3ee51662d32a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2024-01-01 18:01:46.080826')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Timestamp.today() - pd.tseries.offsets.BDay(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1779bc6c-a0ba-4c6b-8e22-8138fe1ab126",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MORN: No price data found, symbol may be delisted (1m 2024-01-01 -> 2024-01-02)\n"
     ]
    }
   ],
   "source": [
    "today_date = pd.to_datetime('today').strftime(\"%Y-%m-%d\")\n",
    "last_bday = (pd.Timestamp.today() - pd.tseries.offsets.BDay(1)).strftime(\"%Y-%m-%d\")\n",
    "morn_price = yf.Ticker(\"MORN\").history(start = last_bday, end = today_date, interval=\"1m\")[\"Close\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce0cf85d-4a8c-4451-91b2-2ffeef4e6175",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "<class 'pandas.core.indexes.base.Index'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m morn_price_full \u001b[38;5;241m=\u001b[39m \u001b[43mmorn_price\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masfreq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(morn_price_full)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pymc_env\\lib\\site-packages\\pandas\\core\\generic.py:8878\u001b[0m, in \u001b[0;36mNDFrame.asfreq\u001b[1;34m(self, freq, method, how, normalize, fill_value)\u001b[0m\n\u001b[0;32m   8771\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   8772\u001b[0m \u001b[38;5;124;03mConvert time series to specified frequency.\u001b[39;00m\n\u001b[0;32m   8773\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   8874\u001b[0m \u001b[38;5;124;03m2000-01-01 00:03:00    3.0\u001b[39;00m\n\u001b[0;32m   8875\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   8876\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresample\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m asfreq\n\u001b[1;32m-> 8878\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43masfreq\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   8879\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8880\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfreq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8885\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pymc_env\\lib\\site-packages\\pandas\\core\\resample.py:2686\u001b[0m, in \u001b[0;36masfreq\u001b[1;34m(obj, freq, method, how, normalize, fill_value)\u001b[0m\n\u001b[0;32m   2683\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(obj\u001b[38;5;241m.\u001b[39mindex) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   2684\u001b[0m     new_obj \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m-> 2686\u001b[0m     new_obj\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m=\u001b[39m \u001b[43m_asfreq_compat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2687\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2688\u001b[0m     dti \u001b[38;5;241m=\u001b[39m date_range(obj\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mmin(), obj\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mmax(), freq\u001b[38;5;241m=\u001b[39mfreq)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pymc_env\\lib\\site-packages\\pandas\\core\\resample.py:2723\u001b[0m, in \u001b[0;36m_asfreq_compat\u001b[1;34m(index, freq)\u001b[0m\n\u001b[0;32m   2721\u001b[0m     new_index \u001b[38;5;241m=\u001b[39m TimedeltaIndex([], dtype\u001b[38;5;241m=\u001b[39mindex\u001b[38;5;241m.\u001b[39mdtype, freq\u001b[38;5;241m=\u001b[39mfreq, name\u001b[38;5;241m=\u001b[39mindex\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   2722\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m-> 2723\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;28mtype\u001b[39m(index))\n\u001b[0;32m   2724\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_index\n",
      "\u001b[1;31mTypeError\u001b[0m: <class 'pandas.core.indexes.base.Index'>"
     ]
    }
   ],
   "source": [
    "morn_price_full = morn_price.asfreq(\"T\", method='pad')\n",
    "plt.plot(morn_price_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67b1d0e-9923-456a-a43b-b55f4bc68d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(morn_price_full.values, \"ok\", ms=3, alpha=1.0, label=\"observed data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf123f75-68a9-4d50-9730-314c9344458c",
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = morn_price_full.apply(np.log).diff().dropna()\n",
    "returns.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a3a33f-1fc6-4742-b513-43c6668e2201",
   "metadata": {},
   "source": [
    "You can see that the stock does not trade at many given minutes. This is reflected in the returns series, where it is equal to zero at many periods. That means that we have to deal with the issue of missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719301f4-1ce6-4e7a-9e9a-945324d19a8b",
   "metadata": {},
   "source": [
    "This is where we introduce *Gaussian processes*, which is a Bayesian approach for latent variable regression. A state space model with the latent variable as a Gaussian process can be written as\n",
    "\n",
    "$y_t = f(x_t) + \\varepsilon_t$,\n",
    "\n",
    "$f(x) \\sim GP(m(x), k(x,x'))$,\n",
    "\n",
    "where $m(x)$ is a mean function and $k(x,x')$ is a covariance function between points $x$ and $x'$ of the latent variable.\n",
    "\n",
    "Gaussian processes are a type of nonparametric model. Nonparametric models are named like that because they do not have a fixed number of parameters, and instead allow for flexibility letting the number of parameters vary with the data. This is advantageous in our case because we have unevenly observed data, with irregular intervals between observations. In the case of Gaussian processes, the estimated latent variable is the function $f(x)$, which can be evaluated for any value of $x$, including at time periods where we did not have an observation for $y_t$.\n",
    "\n",
    "Now, let's look at the functions $m(x)$ and $k(x,x')$. The first one is a mean function that is generally set to zero or a constant level. The other, $K(x,x')$ is a covariance function between points $x$ and $x'$. Note that there is no fixed length between $x$ and $x'$, but the value of $K$ changes as the distance between both points grow. \n",
    "\n",
    "One example of covariance function for Gaussian processes is the exponentiated quadratic function:\n",
    "\n",
    "$k(x, x') = \\eta^2 * \\mathrm{exp}\\left[ -\\frac{(x - x')^2}{2 \\ell^2} \\right]$,\n",
    "\n",
    "where $\\eta$ and $\\ell$ are parameters. We can see that the covariance is a function of the distance $(x - x')^2$, increasing as the distance shortens. \n",
    "\n",
    "Another covariance function is the exponential function\n",
    "\n",
    "$k(x, x') = \\mathrm{exp}\\left[ -\\frac{||x - x'||}{2\\ell^2} \\right]$,\n",
    "\n",
    "which is not a function of the quadratic deviation $(x - x')^2$ but of the absolute deviation $||x - x'||$.\n",
    "\n",
    "We also have the Matérn 3/2 function:\n",
    "\n",
    "$k(x, x') = \\left(1 + \\frac{\\sqrt{3(x - x')^2}}{\\ell}\\right)\\mathrm{exp}\\left[ - \\frac{\\sqrt{3(x - x')^2}}{\\ell} \\right]$\n",
    "\n",
    "A complete list of Gaussian process covariance functions can be found in: https://www.pymc.io/projects/examples/en/latest/gaussian_processes/GP-MeansAndCovs.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fa13d2-f9ed-444e-9260-d11c833bbf8e",
   "metadata": {},
   "source": [
    "We can then choose one of the functions to be the prior covariance function for our Gaussian process. For our example, let's use the Matérn 3/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d922dc-d4cc-4990-a1bc-0e97218e2c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_sp = np.linspace(0, 10, len(returns))\n",
    "lin_df = pd.Series(lin_sp, index=returns.index, name=\"lin_sp\")\n",
    "X = np.insert(pd.concat([lin_df, morn_price], axis=1).dropna()['lin_sp'].values, 0, 0)[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f73e09a-4c31-4346-b929-1fc04c084f1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with pm.Model() as gp_model:\n",
    "    mean_f = pm.gp.mean.Constant(np.mean(morn_price.values))\n",
    "    lengthscale = 1\n",
    "    cov = 2 * pm.gp.cov.Matern32(1, lengthscale)\n",
    "    gp = pm.gp.Latent(mean_func = mean_f, cov_func=cov)\n",
    "\n",
    "    f = gp.prior(\"f\", X=X)\n",
    "\n",
    "    obs_noise = pm.HalfCauchy(\"sigma\", beta = 1)\n",
    "    y = pm.Normal(\"y\", mu = f, sigma = obs_noise, observed = morn_price.values)\n",
    "pm.model_to_graphviz(gp_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378e6513-ebb8-4ecb-a646-aa7fcd2f87e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gp_model:\n",
    "    trace_gp = pm.sample(5000, tune=2000, return_inferencedata=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c453b7cc-8c73-430f-9a2a-5d0e02886f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_median = np.median(trace_gp.posterior['f'].values, axis=(0,1)) # median across chains and draws \n",
    "gp_hdi_lower = az.hdi(trace_gp, hdi_prob=.95)['f'].T[0].values\n",
    "gp_hdi_upper = az.hdi(trace_gp, hdi_prob=.95)['f'].T[1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc672b51-dc1a-40b9-95d1-17a084b0d52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "morn_price.index.strftime(\"%H:%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7437db05-c1bf-4421-8eee-7cad65b7e615",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25,5))\n",
    "plt.plot(X, gp_median, \"r\", lw=2, label=\"mean and 2σ region\")\n",
    "plt.plot(X, gp_hdi_upper, \"r\", lw=1)\n",
    "plt.plot(X, gp_hdi_lower, \"r\", lw=1)\n",
    "plt.fill_between(X.T[0], gp_hdi_lower, gp_hdi_upper, color=\"r\", alpha=0.5)\n",
    "plt.locator_params(axis='x', nbins=10)\n",
    "plt.xticks(X.T[0], labels=morn_price.index.strftime(\"%H:%M\"), rotation=85)\n",
    "\n",
    "plt.plot(X, morn_price.values, \"ok\", ms=3, alpha=1.0, label=\"observed data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc21334-51af-4fbd-a44b-40291751905b",
   "metadata": {},
   "source": [
    "We can take this purely statistical model for our data, and improve upon it with econometric theory. For example, bid-ask bounce effects might be present in our data because of nonsynchronous trading.\n",
    "\n",
    "Roll's model for the bid-ask spread is\n",
    "\n",
    "$P_t = P_t^* + I_t \\frac{s}{2}$,\n",
    "\n",
    "$I_t = p * 1 + (1-p) * -1$,\n",
    "\n",
    "where $P_t$ is the observed price, $P_t^*$ is the unobserved fundamental price, $s$ is the spread and $I_t$ is an indicator variable that assumes the values $1$ or $-1$ with 50% probability each. We can incorporate this structure into our Gaussian process model and estimate the bid-ask spread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c559af7c-8e69-44fa-852d-9102bf56ffba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with pm.Model() as gp_model_roll:\n",
    "    \n",
    "    s = pm.Gamma(\"spread\", 2, 5)\n",
    "    p = pm.Binomial(\"p\", n=1, p=0.5)\n",
    "    \n",
    "    I = p * 1 + (1-p) * -1\n",
    "    \n",
    "    mean_f = pm.gp.mean.Constant(np.mean(morn_price.values))\n",
    "    lengthscale = 1\n",
    "    cov = 2 * pm.gp.cov.Matern32(1, lengthscale)\n",
    "    gp = pm.gp.Latent(mean_func = mean_f, cov_func=cov)\n",
    "\n",
    "    f = gp.prior(\"f\", X=X) + (I * (s/2))\n",
    "\n",
    "    obs_noise = pm.HalfCauchy(\"sigma\", beta = 0.1)\n",
    "    y = pm.Normal(\"y\", mu = f, sigma = obs_noise, observed = morn_price.values)\n",
    "pm.model_to_graphviz(gp_model_roll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d479746f-323b-47ae-b00c-47f9c5fe90e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gp_model_roll:\n",
    "    trace_roll = pm.sample(5000, tune=2000, target_accept=0.9, return_inferencedata=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fc8a42-b837-456e-ae74-bd5e2d2d9ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "roll_median = np.median(trace_roll.posterior['f'].values, axis=(0,1)) # median across chains and draws \n",
    "roll_hdi_lower = az.hdi(trace_roll, hdi_prob=.95)['f'].T[0].values\n",
    "roll_hdi_upper = az.hdi(trace_roll, hdi_prob=.95)['f'].T[1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40784f1-66c7-4c3d-aaf4-7e83908b5ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25,5))\n",
    "plt.plot(X, roll_median, \"r\", lw=2, label=\"mean and 2σ region\")\n",
    "plt.plot(X, roll_hdi_upper, \"r\", lw=1)\n",
    "plt.plot(X, roll_hdi_lower, \"r\", lw=1)\n",
    "plt.fill_between(X.T[0], roll_hdi_lower, roll_hdi_upper, color=\"r\", alpha=0.5)\n",
    "plt.locator_params(axis='x', nbins=10)\n",
    "plt.xticks(X.T[0], labels=morn_price.index.strftime(\"%H:%M\"), rotation=85)\n",
    "\n",
    "plt.plot(X, morn_price.values, \"ok\", ms=3, alpha=1.0, label=\"observed data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc2dace-bf63-44e8-a88b-531f1ec12cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(trace_roll, var_names=[\"spread\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50611f83-76f5-4051-a44e-7f2d538b4a34",
   "metadata": {},
   "source": [
    "# Calculation of dynamic VaR/ES with stochastic volatility from the posterior predictive distribution\n",
    "\n",
    "Value at risk (VaR) and expected shortfall (ES) are standard measures of risk for financial assets/portfolios. The former is the worst possible loss at a confidence level $\\alpha$ (e.g. 95%) and the latter is the mean loss given that one of the $1 - \\alpha$ worst outcomes has happened.\n",
    "\n",
    "VaR and ES can be obtained from historical returns or parametric/Monte Carlo approaches, and most common implementations assume constant volatility over time. Using Bayesian methods we can fit a model with time-varying volatility to the historical returns data, and see how VaR/ES changes along time.\n",
    "\n",
    "Stochastic volatility (SV) models are a type of state space model, where the volatility is a latent variable. As the volatility is the standard deviation of returns, and it can never assume negative values, a SV model has the general specification\n",
    "\n",
    "$y_t \\sim N(0, \\exp \\{ v_t \\})$,\n",
    "\n",
    "$v_t = v_{t-1} + \\varepsilon_t$,\n",
    "\n",
    "where $y_t$ are the returns and $v_t$ is known as the log-volatility. The functional form for the log-volatility above is a random walk, but other processes can also be used.\n",
    "\n",
    "Let's try fitting a SV model where the log-volatility follows a random walk to a series of returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4c6600-c3b0-4913-9b86-8844575068e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "amzn_price = yf.Ticker(\"AMZN\").history(start = \"2019-01-01\")[\"Close\"]\n",
    "amzn_returns = amzn_price.apply(np.log).diff().dropna()\n",
    "plt.plot(amzn_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b8c82b-9456-4134-825c-5676783a01a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as bayesian_sv:\n",
    "    sigma_vol = pm.HalfCauchy(\"sigma_vol\", 1)\n",
    "    volatility = pm.GaussianRandomWalk(\"volatility\", sigma = sigma_vol, shape = len(amzn_returns))\n",
    "    \n",
    "    y = pm.Normal(\"y\", mu = 0, sigma = pm.math.exp(volatility), observed = amzn_returns.values)\n",
    "pm.model_to_graphviz(bayesian_sv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d7ee8d-9d09-4ae7-b5e0-e9298cf2b742",
   "metadata": {},
   "outputs": [],
   "source": [
    "with bayesian_sv:\n",
    "    trace_sv = pm.sample(5000, tune=2000, return_inferencedata=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf81f12-3ef4-4497-afc8-5e86051e9cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_median = np.median(trace_sv.posterior['volatility'].values, axis=(0,1)) # median across chains and draws \n",
    "plt.plot(np.exp(vol_median))\n",
    "plt.title(\"Median posterior volatility\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e475c7-ba42-4f85-8afb-d07117d0402b",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(trace_sv, var_names=[\"~volatility\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa3109c-e00e-4b24-919f-ce2dca20864e",
   "metadata": {},
   "source": [
    "Now, instead of a random walk, we can try to specify the log-volatility process as an AR(1), which is common in the literature (https://dspace.cuni.cz/bitstream/handle/20.500.11956/40737/DPTX_2010_2__0_323248_0_111009.pdf?sequence=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cf54ab-0a76-41f2-840b-dd20ae92705e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as bayesian_sv_ar:\n",
    "    phi = pm.Beta(\"phi\", 1, 1, shape = 2)\n",
    "    \n",
    "    sigma_vol = pm.HalfCauchy(\"sigma_vol\", 1)\n",
    "    volatility = pm.AR(\"volatility\", rho = phi, sigma = sigma_vol, constant=True, shape = len(amzn_returns))\n",
    "\n",
    "    y = pm.Normal(\"y\", mu = 0, sigma = np.exp(volatility), observed = amzn_returns.values)\n",
    "pm.model_to_graphviz(bayesian_sv_ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b069f22-e856-4fe8-b602-b240efecd0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with bayesian_sv_ar:\n",
    "    trace_sv_ar = pm.sample(5000, tune=2000, return_inferencedata=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f567eed1-c243-488c-8859-6ee2ca880d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_median = np.median(trace_sv_ar.posterior['volatility'].values, axis=(0,1)) # median across chains and draws \n",
    "plt.plot(np.exp(vol_median))\n",
    "plt.title(\"Median posterior volatility\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5c2730-9dfd-42ea-a1fe-2948b916915c",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(trace_sv_ar, var_names=[\"~volatility\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d91c53-ac9e-4fc8-898f-24e02ddf206b",
   "metadata": {},
   "source": [
    "It seems our AR coefficient (orange) heavily tends towards 1, which can be interpreted as evidence that the random walk assumption for our log-volatility seems valid.\n",
    "\n",
    "We can try to specify models with jumps for the returns. Jumps are discrete changes at specific time periods, and can emulate movements such as the market reacting to new information. Statistical models for jumps generally have two components: the jump size and an indicator variable of whether a jump has happened or not.\n",
    "\n",
    "A possible specification for jumps (https://joshuachan.org/papers/energy_GARCH_SV.pdf) is $q_t k_t$, where $q_t$ is a binary outcome variable (0 or 1) representing whether a jump happens or not at that period (with probability $\\pi$), and $k_t$ is the size of the jump."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a70997-ce1b-4ee5-9b92-7fd35158f7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as bayesian_sv_jump:\n",
    "    sigma_vol = pm.HalfCauchy(\"sigma_vol\", 1)\n",
    "    volatility = pm.GaussianRandomWalk(\"volatility\", sigma = sigma_vol, shape = len(amzn_returns))\n",
    "        \n",
    "    pi = pm.Beta('pi', 1, 1)\n",
    "    jump_size = pm.Gamma(\"jump size\", 2, 5, shape=len(amzn_returns))\n",
    "    jump_occ = pm.Binomial(\"jump occurrence\", p=pi, n=1, shape=len(amzn_returns))\n",
    "    jump = jump_size * jump_occ\n",
    "\n",
    "    y = pm.Normal(\"y\", mu = jump, sigma = pm.math.exp(volatility), observed = amzn_returns.values)\n",
    "pm.model_to_graphviz(bayesian_sv_jump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca38050-688e-428c-bec1-0f5580cb1cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with bayesian_sv_jump:\n",
    "    trace_sv_jump = pm.sample(5000, tune=2000, return_inferencedata=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa99ad2-88ce-43dc-814d-734ff122387f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_median = np.median(trace_sv_jump.posterior['volatility'].values, axis=(0,1)) # median across chains and draws \n",
    "plt.plot(np.exp(vol_median))\n",
    "plt.title(\"Median posterior volatility\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871f4f3c-108d-454d-b4fc-a3bd1ad59463",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(trace_sv_jump, var_names=[\"~volatility\",'~jump size'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234b5976-142b-4794-bebb-2c039c25568e",
   "metadata": {},
   "source": [
    "The estimated jump probability (pi) tends heavily towards zero, so we conclude that including jumps did not provide significant gains for our model.\n",
    "\n",
    "Lastly, Bayesian computation also allows us to specify the returns as coming from a t distribution, so we can also try that and assign a prior to the degrees of freedom parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1ade67-71c9-4039-a357-ca1f360d0c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as bayesian_sv_tdist:\n",
    "    sigma_vol = pm.HalfCauchy(\"sigma_vol\", 1)\n",
    "    volatility = pm.GaussianRandomWalk(\"volatility\", sigma = sigma_vol, shape = len(amzn_returns))\n",
    "\n",
    "    nu = pm.Exponential('nu', 1)\n",
    "    y = pm.StudentT(\"y\", nu=nu, mu = 0, sigma = pm.math.exp(volatility), observed = amzn_returns.values)\n",
    "pm.model_to_graphviz(bayesian_sv_tdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4227e234-2353-446a-a9dd-d3a273f9cdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with bayesian_sv_tdist:\n",
    "    trace_sv_tdist = pm.sample(5000, tune=2000, return_inferencedata=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f909152c-1b37-4732-a3ef-bd48f9ed919d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_median = np.median(trace_sv_tdist.posterior['volatility'].values, axis=(0,1)) # median across chains and draws \n",
    "plt.plot(np.exp(vol_median))\n",
    "plt.title(\"Median posterior volatility\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d241234-b38e-4cc0-ae0c-bc3380fd62f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(trace_sv_tdist, var_names=[\"~volatility\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef151eb-73d6-486d-a89b-314930208cc5",
   "metadata": {},
   "source": [
    "Now that we have a few stochastic models at hand, we can pick one to calculate our value-at-risk/expected shortfall from. For example purposes, we shall pick the t distribution model.\n",
    "\n",
    "Remember the Bayesian inference procedure for parameter(s) $\\theta$ and a set of data $y$:\n",
    "\n",
    "$P(\\theta|y) = \\frac{P(\\theta)P(y|\\theta)}{P(y)}$.\n",
    "\n",
    "A Bayesian model is defined as the joint distribuion of the data and all parameters. This means that, not only we can find the posterior distribution of the parameters given the priors and a set of data, we can also use the posterior obtained from a first set of data to sample unobserved future sets of data from the model. The distribution of possible future values of data given the posterior from a previous set of data is known as the *posterior predictive* distribution. \n",
    "\n",
    "Given a first set of data $y$, a future set of data $\\tilde{y}$ can be obtained as\n",
    "\n",
    "$P(\\tilde{y}|y) = \\int_\\Theta P(\\tilde{y}|\\theta,y)P(\\theta|y)d\\theta$,\n",
    "\n",
    "which is a marginalization over the parameter(s) $\\theta$.\n",
    "\n",
    "We can use our model to generate a set of posterior predictive data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca005da-3a58-4571-95a7-55b51cf8bedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with bayesian_sv_tdist:\n",
    "    trace_sv_tdist.extend(pm.sample_posterior_predictive(trace_sv_tdist))\n",
    "sv_ppd = trace_sv_tdist.posterior_predictive.stack(pooled_chain=(\"chain\", \"draw\"))\n",
    "pp_returns = sv_ppd.y.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d00757-71a2-4a56-acd8-eb8176e42f8d",
   "metadata": {},
   "source": [
    "The posterior predictive simulation generates 20000 samples for each period of our time series of returns. We can then calculate VaR and ES for the 20000 samples at each time period in order to obtain a dynamic VaR/ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc4689b-8f1c-47f3-b52c-2ef6e3156cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_level = 0.95\n",
    "var = np.quantile(pp_returns, 1 - conf_level, axis=1)\n",
    "es = np.empty(len(amzn_returns))\n",
    "for i in range(0,len(amzn_returns)):\n",
    "    tail = pp_returns[i][(pp_returns[i] < np.quantile(pp_returns[i], 1 - conf_level))]\n",
    "    mean_tail = np.mean(tail)\n",
    "    es[i] = mean_tail\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(amzn_returns.values, label=\"AMZN returns\")\n",
    "plt.plot(var, color=\"r\", label=\"5th percentile of returns (VaR)\")\n",
    "plt.plot(es, color=\"k\", linestyle=\"dashed\", label=\"Mean return given worst 5% (ES)\")\n",
    "plt.legend()\n",
    "plt.title(\"Posterior predictive VaR and ES\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9636e09-245e-4973-af97-b7c2e3a9ca84",
   "metadata": {},
   "source": [
    "We can compare this dynamic VaR/ES obtained from the model with t-distributed returns and stochastic volatility to that of a model with normally distributed returns and constant volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fee5407-2dcb-4d3a-85de-c8046d528b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as normal_const_vol:\n",
    "    volatility = pm.HalfCauchy(\"volatility\", 1)\n",
    "    y = pm.Normal('y', mu = 0, sigma = volatility, observed = amzn_returns.values)\n",
    "\n",
    "with normal_const_vol:\n",
    "    trace_normal_cv = pm.sample(5000, tune=2000, return_inferencedata=True)\n",
    "    trace_normal_cv.extend(pm.sample_posterior_predictive(trace_normal_cv))\n",
    "nc_ppd = trace_normal_cv.posterior_predictive.stack(pooled_chain=(\"chain\", \"draw\"))\n",
    "pp_returns_nc = nc_ppd.y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8a712c-4130-4378-8b63-a65ea64d3e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_level = 0.95\n",
    "var_nc = np.quantile(pp_returns_nc, 1 - conf_level, axis=1)\n",
    "es_nc = np.empty(len(amzn_returns))\n",
    "for i in range(0,len(amzn_returns)):\n",
    "    tail = pp_returns_nc[i][(pp_returns_nc[i] < np.quantile(pp_returns_nc[i], 1 - conf_level))]\n",
    "    mean_tail = np.mean(tail)\n",
    "    es_nc[i] = mean_tail\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(amzn_returns.values, label=\"AMZN returns\")\n",
    "plt.plot(var, color=\"r\", label=\"VaR t + SV\")\n",
    "plt.plot(es, color=\"k\", linestyle=\"dashed\", label=\"ES t + SV\")\n",
    "plt.plot(var_nc, color=\"purple\", linestyle=\"dotted\", label=\"VaR normal\")\n",
    "plt.plot(es_nc, color=\"darkorange\", linestyle=\"dashdot\", label=\"ES normal\")\n",
    "plt.legend()\n",
    "plt.title(\"VaR and ES: t + SV vs. Normal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260e9cea-744a-4b8b-bda1-4a148da1f7a3",
   "metadata": {},
   "source": [
    "For the normal model, the VaR/ES is constant because the posterior predictive returns at each time are sampled from a distribution with constant volatility. This can underestimate risk at high volatility periods and superestimate risk at low volatility periods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0522af-628c-4e8f-b2fd-38690b277387",
   "metadata": {},
   "source": [
    "## Additional references for this section\n",
    "\n",
    "Gaussian Processes for Time Series Forecasting with PyMC by Juan Orduz: https://juanitorduz.github.io/gp_ts_pymc3/.\n",
    "\n",
    "Campbell, John Y., Andrew W. Lo, and A. Craig MacKinlay. The Econometrics of Financial Markets. Princeton University Press, 1997.\n",
    "\n",
    "Roll, Richard. \"A Simple Implicit Measure of the Effective Bid-Ask Spread in an Efficient Market\" The Journal of Finance 39-4 (1984): 1127-1139.\n",
    "\n",
    "Tesarova, Viktoria. \"Value at Risk: GARCH vs. Stochatistic Volatility Models: Empirical Study.\" (2013).\n",
    "\n",
    "Chan, Joshua CC, and Angelia L. Grant. \"Modeling energy price dynamics: GARCH versus stochastic volatility.\" Energy Economics 54 (2016): 182-189."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
