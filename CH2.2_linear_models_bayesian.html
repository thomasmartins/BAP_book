

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Linear models: classic and Bayesian approaches &#8212; Bayesian Methods in Asset Pricing</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'CH2.2_linear_models_bayesian';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Bayesian techniques for factor models" href="CH2.3_hierarchical_bayesian_capm.html" />
    <link rel="prev" title="The cross-section of stock returns" href="CH2.1_cross_section_factors.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="landing_intro.html">
  
  
  
  
    
    
      
    
    
    <img src="_static/BAP_logo.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="_static/BAP_logo.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="landing_intro.html">
                    Introduction
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="CH1_allocation.html">Chapter 1: Risk, return and allocation</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="CH1.1_returns_mvo.html">Returns, risk and diversification</a></li>
<li class="toctree-l2"><a class="reference internal" href="CH1.2_bayes.html">Bayesian inference and computation</a></li>
<li class="toctree-l2"><a class="reference internal" href="CH1.3_black_litterman.html">Black-Litterman model for portfolio allocation</a></li>
</ul>
</li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="CH2_cross_section.html">Chapter 2: The cross section of asset returns</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="CH2.1_cross_section_factors.html">The cross-section of stock returns</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Linear models: classic and Bayesian approaches</a></li>
<li class="toctree-l2"><a class="reference internal" href="CH2.3_hierarchical_bayesian_capm.html">Bayesian techniques for factor models</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="CH3_time_series.html">Chapter 3: Time series</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="CH3.1_timeseries_stockprices.html">Time series of stock prices and returns</a></li>
<li class="toctree-l2"><a class="reference internal" href="CH3.2_bayesian_timeseries.html">Time series modeling: classical and Bayesian approaches</a></li>
<li class="toctree-l2"><a class="reference internal" href="CH3.3_ARIMA_SV_ES_VAR.html">Bayesian state space approach for nonsynchronous trading</a></li>

</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="ref_chapter.html">References</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/thomasmartins/BAP_book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/thomasmartins/BAP_book/issues/new?title=Issue%20on%20page%20%2FCH2.2_linear_models_bayesian.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/CH2.2_linear_models_bayesian.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Linear models: classic and Bayesian approaches</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#flat-versus-regularizing-priors">Flat versus regularizing priors</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#generalized-linear-models-glm">Generalized linear models (GLM)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multilevel-modelling">Multilevel modelling</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#additional-references-for-this-section">Additional references for this section</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="linear-models-classic-and-bayesian-approaches">
<h1>Linear models: classic and Bayesian approaches<a class="headerlink" href="#linear-models-classic-and-bayesian-approaches" title="Permalink to this heading">#</a></h1>
<p>In statistics, a linear model, or linear regression, is one way of describing the relationship between two variables. If we want to describe variable <span class="math notranslate nohighlight">\(Y\)</span> as a function of variable <span class="math notranslate nohighlight">\(X\)</span>, a linear model would look like the following</p>
<p><span class="math notranslate nohighlight">\(y_i = \alpha + \beta x_i + \varepsilon_i\)</span>,</p>
<p>where <span class="math notranslate nohighlight">\(\alpha\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> are, respectively, the intercept and coefficient, and <span class="math notranslate nohighlight">\(\varepsilon\)</span> is a normal random variable with mean 0 and some variance <span class="math notranslate nohighlight">\(\sigma^2\)</span>.</p>
<p>For each observation <span class="math notranslate nohighlight">\(i = 1, 2, \ldots, N\)</span> we observe values for <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>. Then, a statistical estimation procedure, such as least squares, is employed for obtaining estimates for the parameters <span class="math notranslate nohighlight">\(\alpha\)</span>, <span class="math notranslate nohighlight">\(\beta\)</span> and <span class="math notranslate nohighlight">\(\sigma^2\)</span>.</p>
<p>The parameters have the following interpretations: the intercept <span class="math notranslate nohighlight">\(\alpha\)</span> is the value <span class="math notranslate nohighlight">\(y_i\)</span> would have, on average, if <span class="math notranslate nohighlight">\(x_i\)</span> was set to zero; the coefficient <span class="math notranslate nohighlight">\(\beta\)</span> is the size a change in <span class="math notranslate nohighlight">\(y_i\)</span> would have if <span class="math notranslate nohighlight">\(x_i\)</span> was changed by 1; and <span class="math notranslate nohighlight">\(\sigma^2\)</span> is the variance of the error.</p>
<p>The error is the portion of <span class="math notranslate nohighlight">\(y_i\)</span> that is not explained by <span class="math notranslate nohighlight">\(x_i\)</span>, or the difference between the observed and expected values for <span class="math notranslate nohighlight">\(y_i\)</span>, based on the population mean. A closely related variable is the residual, which is conceptually similar to the error, but based on statistics estimated from the sample, and not the whole population, e.g., the sample mean instead of the population mean. The residuals are observable, and the errors are not. The residuals are an estimate of the error.</p>
<p>Our example is an univariate, simple linear regression, but it can be extended to multivariate models.</p>
<p>One of the most common uses of linear regression is prediction. Having estimated the parameters with an observed dataset for <span class="math notranslate nohighlight">\(Y\)</span> and <span class="math notranslate nohighlight">\(X\)</span>, if we have new observations with values for <span class="math notranslate nohighlight">\(X\)</span>, we could use the estimated parameters to predict what the values for <span class="math notranslate nohighlight">\(Y\)</span> would be, on average.</p>
<p>There is an uncanny relationship between linear regression and hypothesis testing, two of the most important tools from introductory statistics. Most parametric and non-parametric tests from classical statistics can be visualized as special cases of regression. See: <a class="reference external" href="https://lindeloev.github.io/tests-as-linear/">https://lindeloev.github.io/tests-as-linear/</a></p>
<p>For example, a simple t-test for the sample mean of a normal distribution can be imagined as a statistical test to see if the sample mean is an useful <em>predictor</em> of the observed data. As there are no covariates <span class="math notranslate nohighlight">\(X\)</span>, it is similar to regressing the values for <span class="math notranslate nohighlight">\(Y\)</span> on a column of ones. The intercept of this model is the sample mean.</p>
<p>In classical statistics, it is commonplace to perform t-tests for the regression parameters, and also an F-test for overall model significance.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span> 
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="o">%</span><span class="k">matplotlib</span> inline

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="mi">50</span> <span class="o">+</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">70</span><span class="p">)</span> <span class="o">*</span> <span class="mi">20</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="mi">20</span> <span class="o">+</span> <span class="p">(</span><span class="mf">0.8</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">70</span><span class="p">)</span> <span class="o">*</span> <span class="mi">10</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;y&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0.5, 0, &#39;x&#39;)
</pre></div>
</div>
<img alt="_images/fc92bce059899d0b6ff57e9e8363d0be09de29517c982aac1fe5a4b69522757e.png" src="_images/fc92bce059899d0b6ff57e9e8363d0be09de29517c982aac1fe5a4b69522757e.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">statsmodels.api</span> <span class="k">as</span> <span class="nn">sm</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="c1"># add_constant so that our model has an intercept</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">results</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       0.744
Model:                            OLS   Adj. R-squared:                  0.741
Method:                 Least Squares   F-statistic:                     197.9
Date:                Thu, 11 Jan 2024   Prob (F-statistic):           8.11e-22
Time:                        09:24:52   Log-Likelihood:                -265.40
No. Observations:                  70   AIC:                             534.8
Df Residuals:                      68   BIC:                             539.3
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
const         20.1805      3.197      6.312      0.000      13.801      26.560
x1             0.7962      0.057     14.069      0.000       0.683       0.909
==============================================================================
Omnibus:                        1.259   Durbin-Watson:                   2.225
Prob(Omnibus):                  0.533   Jarque-Bera (JB):                1.092
Skew:                           0.102   Prob(JB):                        0.579
Kurtosis:                       2.423   Cond. No.                         139.
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</pre></div>
</div>
</div>
</div>
<p>From the previous chapter, we already know the conceptual differences between classical and Bayesian statistics. The way Bayesians account for parameter uncertainty is one of the most important of them.</p>
<p>So how does the Bayesian interpretation of a linear model differ from that of classical statistics?</p>
<p>Since our error variable, <span class="math notranslate nohighlight">\(\varepsilon\)</span>, has a normal distribution</p>
<p><span class="math notranslate nohighlight">\(\varepsilon \sim N(0, \sigma^2)\)</span>,</p>
<p>and our observed values for <span class="math notranslate nohighlight">\(X\)</span> are given, we can also write <span class="math notranslate nohighlight">\(Y\)</span> as a normally distributed random variable</p>
<p><span class="math notranslate nohighlight">\(Y \sim N(\alpha + \beta X, \sigma^2)\)</span>.</p>
<p>If we change <span class="math notranslate nohighlight">\(\alpha + \beta X\)</span> to multivariate notation for <span class="math notranslate nohighlight">\(k\)</span> covariates, and write it as <span class="math notranslate nohighlight">\(\mathbf{x}' \boldsymbol{\beta}\)</span>, with the first column of <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> being a column of ones (so that the model has an intercept), we now have a vector <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> of length <span class="math notranslate nohighlight">\(k+1\)</span>, containing the intercept plus coefficients for the <span class="math notranslate nohighlight">\(k\)</span> covariates. We can also assume the covariance matrix for <span class="math notranslate nohighlight">\(Y\)</span> to be of the form <span class="math notranslate nohighlight">\(\sigma^2 I\)</span>, where <span class="math notranslate nohighlight">\(I\)</span> is an identity matrix. In this case our model has a total of <span class="math notranslate nohighlight">\(k+2\)</span> parameters.</p>
<p>We could assume conjugate prior distributions for these parameters. In this case the vector <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> would have a multivariate normal distribution and <span class="math notranslate nohighlight">\(\sigma^2\)</span> would have an inverse-gamma distribution.</p>
<p>We could also use approximation methods such as MCMC to have more flexibility while specifying priors. Either way, Bayesian inference treats the parameters as random variables.</p>
<p>Here we shall estimate a model via MCMC using flat priors, i.e., priors with a very large standard deviation</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pymc</span> <span class="k">as</span> <span class="nn">pm</span>
<span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="nn">az</span>

<span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">linear_model</span><span class="p">:</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;alpha&#39;</span><span class="p">,</span> <span class="n">mu</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span> <span class="o">=</span> <span class="mf">1e4</span><span class="p">)</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;beta&#39;</span><span class="p">,</span> <span class="n">mu</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span> <span class="o">=</span> <span class="mf">1e4</span><span class="p">)</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">HalfCauchy</span><span class="p">(</span><span class="s1">&#39;sigma&#39;</span><span class="p">,</span> <span class="n">beta</span> <span class="o">=</span> <span class="mf">1e4</span><span class="p">)</span>
    <span class="n">y_obs</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">mu</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">+</span> <span class="p">(</span><span class="n">beta</span> <span class="o">*</span> <span class="n">x</span><span class="p">),</span> <span class="n">sigma</span> <span class="o">=</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">observed</span> <span class="o">=</span> <span class="n">y</span><span class="p">)</span>

    <span class="n">trace_lm</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">2000</span><span class="p">,</span> <span class="n">tune</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">target_accept</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s1">&#39;adapt_diag&#39;</span><span class="p">)</span>
<span class="n">az</span><span class="o">.</span><span class="n">summary</span><span class="p">(</span><span class="n">trace_lm</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>WARNING (pytensor.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Auto-assigning NUTS sampler...
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Initializing NUTS using adapt_diag...
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Multiprocess sampling (4 chains in 4 jobs)
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>NUTS: [alpha, beta, sigma]
</pre></div>
</div>
<div class="output text_html">
    <div>
        <style>
            /* Turns off some styling */
            progress {
                /* gets rid of default border in Firefox and Opera. */
                border: none;
                /* Needs to be in here for Safari polyfill so background images work as expected. */
                background-size: auto;
            }
            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
                background: #F44336;
            }
        </style>
      <progress value='12000' class='' max='12000' style='width:300px; height:20px; vertical-align: middle;'></progress>
      100.00% [12000/12000 00:22<00:00 Sampling 4 chains, 0 divergences]
    </div>
    </div><div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Sampling 4 chains for 1_000 tune and 2_000 draw iterations (4_000 + 8_000 draws total) took 39 seconds.
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>mean</th>
      <th>sd</th>
      <th>hdi_3%</th>
      <th>hdi_97%</th>
      <th>mcse_mean</th>
      <th>mcse_sd</th>
      <th>ess_bulk</th>
      <th>ess_tail</th>
      <th>r_hat</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>alpha</th>
      <td>20.209</td>
      <td>3.239</td>
      <td>13.887</td>
      <td>25.987</td>
      <td>0.058</td>
      <td>0.041</td>
      <td>3127.0</td>
      <td>3289.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>beta</th>
      <td>0.795</td>
      <td>0.057</td>
      <td>0.685</td>
      <td>0.902</td>
      <td>0.001</td>
      <td>0.001</td>
      <td>3115.0</td>
      <td>3394.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>sigma</th>
      <td>11.115</td>
      <td>0.978</td>
      <td>9.271</td>
      <td>12.857</td>
      <td>0.015</td>
      <td>0.011</td>
      <td>4562.0</td>
      <td>4332.0</td>
      <td>1.0</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">az</span><span class="o">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">trace_lm</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[&lt;Axes: title={&#39;center&#39;: &#39;alpha&#39;}&gt;,
        &lt;Axes: title={&#39;center&#39;: &#39;alpha&#39;}&gt;],
       [&lt;Axes: title={&#39;center&#39;: &#39;beta&#39;}&gt;,
        &lt;Axes: title={&#39;center&#39;: &#39;beta&#39;}&gt;],
       [&lt;Axes: title={&#39;center&#39;: &#39;sigma&#39;}&gt;,
        &lt;Axes: title={&#39;center&#39;: &#39;sigma&#39;}&gt;]], dtype=object)
</pre></div>
</div>
<img alt="_images/b5f1d90200692681994710d01556a7b8c283b0ff4a37ea8e907b5e8502e440c7.png" src="_images/b5f1d90200692681994710d01556a7b8c283b0ff4a37ea8e907b5e8502e440c7.png" />
</div>
</div>
<section id="flat-versus-regularizing-priors">
<h2>Flat versus regularizing priors<a class="headerlink" href="#flat-versus-regularizing-priors" title="Permalink to this heading">#</a></h2>
<p>Tools from Bayesian data analysis can be helpful in specifying priors. We must always be mindful of the implications for the chosen prior distributions, even “noninformative” ones.</p>
<p>For example, suppose we want to fit a linear model to analyse the relationship between height and weight, for example. If we specify weight (in kg) as a function of height (in cm), a coefficient of 1.5 would mean an increase of 1 cm in height would average an increase of 1.5 kg in weight. It is very important to remark that linear models in many cases do not have a causal interpretation, but, for simplicity, suppose there is a causal relationship between the two.</p>
<p>Suppose we pick the following “noninformative” prior for our regression coefficient</p>
<p><span class="math notranslate nohighlight">\(\beta \sim U(-\infty, \infty)\)</span>.</p>
<p>This prior distribution has the implication that, if we draw a value from the prior distribution, every real number has the same probability of being observed. For example, suppose we draw from the prior distribution of <span class="math notranslate nohighlight">\(\beta\)</span> the two following values: -5000, 2. Would you say, before observing any data, that those two values are equally likely to be the true value of the parameter in the underlying data generating process?</p>
<p>A Bayesian data analysis technique called prior predictive checking can help us observe the implications of our chosen priors for the parameters even in situations where those might be difficult to imagine, e.g., complex models with tens of parameters.</p>
<p>This can also be helpful for variable selection. Even a frequentist statistician might think, a priori, that linear models with an excess number of covariates might lead to things like spurious relationships and/or overfitting. In this case, said statistician would recur to regularization techniques.</p>
<p>Bayesian inference via MCMC makes it easy to impose regularization via prior distributions on the coefficients. Regularizing priors, or shrinkage priors, can be imposed via distributions that have more mass around zero. The Laplace prior on a regression coefficient corresponds to L1 regularization (LASSO) and the normal prior to L2 regularization (ridge).</p>
<p>Suppose we have three predictors, <span class="math notranslate nohighlight">\(x_1\)</span>, <span class="math notranslate nohighlight">\(x_2\)</span> and <span class="math notranslate nohighlight">\(x_3\)</span>, but only <span class="math notranslate nohighlight">\(x_1\)</span> has a causal relationship with <span class="math notranslate nohighlight">\(y\)</span>. Let’s estimate a linear model with all three predictors, using regularizing priors for the coefficients.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scipy</span> <span class="k">as</span> <span class="nn">sp</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">50</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">sp</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Regularizing&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">sp</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">1e4</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Flat&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Priors for coefficients: regularizing versus flat&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/5176b2d79cc82e340603e5de478a9cf5396a56c4c333d08c416981c663695550.png" src="_images/5176b2d79cc82e340603e5de478a9cf5396a56c4c333d08c416981c663695550.png" />
</div>
</div>
<p>It is possible to control the effect of the regularization (i.e. how much the coefficient shrinks towards zero) via the standard deviation hyperparameter.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x1</span> <span class="o">=</span> <span class="mi">50</span> <span class="o">+</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span> <span class="o">*</span> <span class="mi">20</span><span class="p">)</span>
<span class="n">x2</span> <span class="o">=</span> <span class="mi">30</span> <span class="o">+</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span> <span class="o">*</span> <span class="mi">25</span><span class="p">)</span>
<span class="n">x3</span> <span class="o">=</span> <span class="mi">65</span> <span class="o">+</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span> <span class="o">*</span> <span class="mi">18</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="mi">20</span> <span class="o">+</span> <span class="p">(</span><span class="mf">0.4</span> <span class="o">*</span> <span class="n">x1</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span> <span class="o">*</span> <span class="mi">10</span><span class="p">)</span>

<span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">reg_model</span><span class="p">:</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;alpha&#39;</span><span class="p">,</span> <span class="n">mu</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span> <span class="o">=</span> <span class="mf">1e4</span><span class="p">)</span>
    <span class="n">beta_1</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;beta_1&#39;</span><span class="p">,</span> <span class="n">mu</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">beta_2</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;beta_2&#39;</span><span class="p">,</span> <span class="n">mu</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">beta_3</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;beta_3&#39;</span><span class="p">,</span> <span class="n">mu</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">HalfCauchy</span><span class="p">(</span><span class="s1">&#39;sigma&#39;</span><span class="p">,</span> <span class="n">beta</span> <span class="o">=</span> <span class="mf">1e4</span><span class="p">)</span>
    <span class="n">y_obs</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">mu</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">+</span> <span class="p">(</span><span class="n">beta_1</span> <span class="o">*</span> <span class="n">x1</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">beta_2</span> <span class="o">*</span> <span class="n">x2</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">beta_3</span> <span class="o">*</span> <span class="n">x3</span><span class="p">),</span> <span class="n">sigma</span> <span class="o">=</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">observed</span> <span class="o">=</span> <span class="n">y</span><span class="p">)</span>

    <span class="n">trace_reg</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">2000</span><span class="p">,</span> <span class="n">tune</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">target_accept</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s1">&#39;adapt_diag&#39;</span><span class="p">)</span>
<span class="n">az</span><span class="o">.</span><span class="n">summary</span><span class="p">(</span><span class="n">trace_reg</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Auto-assigning NUTS sampler...
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Initializing NUTS using adapt_diag...
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Multiprocess sampling (4 chains in 4 jobs)
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>NUTS: [alpha, beta_1, beta_2, beta_3, sigma]
</pre></div>
</div>
<div class="output text_html">
    <div>
        <style>
            /* Turns off some styling */
            progress {
                /* gets rid of default border in Firefox and Opera. */
                border: none;
                /* Needs to be in here for Safari polyfill so background images work as expected. */
                background-size: auto;
            }
            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
                background: #F44336;
            }
        </style>
      <progress value='12000' class='' max='12000' style='width:300px; height:20px; vertical-align: middle;'></progress>
      100.00% [12000/12000 00:38<00:00 Sampling 4 chains, 0 divergences]
    </div>
    </div><div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Sampling 4 chains for 1_000 tune and 2_000 draw iterations (4_000 + 8_000 draws total) took 53 seconds.
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>mean</th>
      <th>sd</th>
      <th>hdi_3%</th>
      <th>hdi_97%</th>
      <th>mcse_mean</th>
      <th>mcse_sd</th>
      <th>ess_bulk</th>
      <th>ess_tail</th>
      <th>r_hat</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>alpha</th>
      <td>21.658</td>
      <td>4.564</td>
      <td>12.742</td>
      <td>29.751</td>
      <td>0.074</td>
      <td>0.052</td>
      <td>3818.0</td>
      <td>4444.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>beta_1</th>
      <td>0.326</td>
      <td>0.057</td>
      <td>0.221</td>
      <td>0.435</td>
      <td>0.001</td>
      <td>0.001</td>
      <td>5158.0</td>
      <td>5369.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>beta_2</th>
      <td>0.023</td>
      <td>0.042</td>
      <td>-0.056</td>
      <td>0.102</td>
      <td>0.001</td>
      <td>0.000</td>
      <td>5845.0</td>
      <td>4989.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>beta_3</th>
      <td>0.037</td>
      <td>0.059</td>
      <td>-0.073</td>
      <td>0.147</td>
      <td>0.001</td>
      <td>0.001</td>
      <td>4557.0</td>
      <td>4539.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>sigma</th>
      <td>10.022</td>
      <td>0.738</td>
      <td>8.726</td>
      <td>11.446</td>
      <td>0.009</td>
      <td>0.006</td>
      <td>6675.0</td>
      <td>4965.0</td>
      <td>1.0</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</section>
<section id="generalized-linear-models-glm">
<h2>Generalized linear models (GLM)<a class="headerlink" href="#generalized-linear-models-glm" title="Permalink to this heading">#</a></h2>
<p>Perhaps one of the most powerful statistical tools for applied research that not many applied researches know of are generalized linear models, or GLM.</p>
<p>Recall that the standard linear model assumes that your dependent variable <span class="math notranslate nohighlight">\(Y\)</span> follows a normal distribution. Depending on the nature of your variable of interest, this assumption might not be reasonable at all.</p>
<p>For example, if your variable of interest is a proportion, it can only assume values between 0 and 1, or, in case your variable is a counting process, it can only assume values in the positive integers (0, 1, 2…). Some variables cannot assume negative values, or are skewed towards positive values.</p>
<p>GLM lets you specify other distributions for the dependent variable, so long as the distributions are part of an exponential family. Some of the most common include binomial, Poisson and gamma distributions, for example. Logistic and Poisson regressions, as well as the standard linear regression, can be written as special cases of GLM.</p>
<p>Generalized linear models also requires the specification of a link function <span class="math notranslate nohighlight">\(g(.)\)</span>. For a model with dependent variable <span class="math notranslate nohighlight">\(Y\)</span> and covariates <span class="math notranslate nohighlight">\(X\)</span>, a GLM would look like</p>
<p><span class="math notranslate nohighlight">\(\operatorname{E}[Y|X] = \mu\)</span>,</p>
<p><span class="math notranslate nohighlight">\(X \beta = g(\mu)\)</span>.</p>
<p>There is a canonical link function for each distribution, but the specification of the link function might change on each application.</p>
<p>Unlike the standard linear regression, no analytical solutions for the estimators can be found, even in classical statistics. In order to obtain the maximum likelihood estimates, the most common estimation procedure in classical statistics, numerical optimization methods must be used.</p>
<p>In the case of Bayesian inference, no conjugate priors can be found. Therefore, computational methods such as MCMC must be used for Bayesian estimation of GLMs.</p>
<p>For example, a logistic regression with logit link function could look like</p>
<p><span class="math notranslate nohighlight">\(y_i \sim Bin(n = 10, p_i)\)</span>,</p>
<p><span class="math notranslate nohighlight">\(p_i = g^{-1}(\alpha + \beta x_i)\)</span>.</p>
<p>As the link function <span class="math notranslate nohighlight">\(g(.)\)</span> is a logit function, the inverse <span class="math notranslate nohighlight">\(g^{-1}(.)\)</span> is the logistic (or expit) function. The logit function is of the form</p>
<p><span class="math notranslate nohighlight">\(logit(x) = \ln \frac{x}{1-x}\)</span>,</p>
<p>and its inverse, the logistic has the form</p>
<p><span class="math notranslate nohighlight">\(f(x) = \frac{1}{1+e^{-x}}\)</span>.</p>
<p>The logistic function maps values from the real line <span class="math notranslate nohighlight">\((-\infty, \infty)\)</span> to the interval <span class="math notranslate nohighlight">\((0,1)\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">alpha_true</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">beta_true</span> <span class="o">=</span> <span class="mf">0.7</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">150</span><span class="p">)</span>
<span class="n">p</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">special</span><span class="o">.</span><span class="n">expit</span><span class="p">(</span><span class="n">alpha_true</span> <span class="o">+</span> <span class="p">(</span><span class="n">x</span> <span class="o">*</span> <span class="n">beta_true</span><span class="p">))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="n">n</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="n">p</span> <span class="o">=</span> <span class="n">p</span><span class="p">)</span>

<span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">logistic_reg</span><span class="p">:</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;alpha&quot;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;beta&quot;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">y_obs</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Binomial</span><span class="p">(</span><span class="s2">&quot;y&quot;</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">pm</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">invlogit</span><span class="p">(</span><span class="n">alpha</span> <span class="o">+</span> <span class="n">beta</span><span class="o">*</span><span class="n">x</span><span class="p">),</span> <span class="n">observed</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
    <span class="n">trace_logistic</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">2000</span><span class="p">,</span> <span class="n">tune</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">target_accept</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s1">&#39;adapt_diag&#39;</span><span class="p">)</span>
<span class="n">az</span><span class="o">.</span><span class="n">summary</span><span class="p">(</span><span class="n">trace_logistic</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Auto-assigning NUTS sampler...
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Initializing NUTS using adapt_diag...
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Multiprocess sampling (4 chains in 4 jobs)
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>NUTS: [alpha, beta]
</pre></div>
</div>
<div class="output text_html">
    <div>
        <style>
            /* Turns off some styling */
            progress {
                /* gets rid of default border in Firefox and Opera. */
                border: none;
                /* Needs to be in here for Safari polyfill so background images work as expected. */
                background-size: auto;
            }
            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
                background: #F44336;
            }
        </style>
      <progress value='12000' class='' max='12000' style='width:300px; height:20px; vertical-align: middle;'></progress>
      100.00% [12000/12000 00:18<00:00 Sampling 4 chains, 0 divergences]
    </div>
    </div><div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Sampling 4 chains for 1_000 tune and 2_000 draw iterations (4_000 + 8_000 draws total) took 33 seconds.
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>mean</th>
      <th>sd</th>
      <th>hdi_3%</th>
      <th>hdi_97%</th>
      <th>mcse_mean</th>
      <th>mcse_sd</th>
      <th>ess_bulk</th>
      <th>ess_tail</th>
      <th>r_hat</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>alpha</th>
      <td>1.143</td>
      <td>0.064</td>
      <td>1.025</td>
      <td>1.267</td>
      <td>0.001</td>
      <td>0.001</td>
      <td>7080.0</td>
      <td>5927.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>beta</th>
      <td>0.780</td>
      <td>0.068</td>
      <td>0.649</td>
      <td>0.906</td>
      <td>0.001</td>
      <td>0.001</td>
      <td>7109.0</td>
      <td>5798.0</td>
      <td>1.0</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</section>
<section id="multilevel-modelling">
<h2>Multilevel modelling<a class="headerlink" href="#multilevel-modelling" title="Permalink to this heading">#</a></h2>
<p>Another Bayesian approach for linear models is multilevel modelling. This is particularly useful when the data is divided into clusters, and the assumption that the data in one cluster contains information about the other clusters is valid.</p>
<p>Also called hierarchical modelling or partial pooling, multilevel modelling provides more flexibility in such cases. It departs from both the assumptions that one cluster does not contain any information at all about others, or that the data in all clusters are drawn from the same distribution.</p>
<p>An example of Bayesian multilevel model would be</p>
<p><span class="math notranslate nohighlight">\(Y_{i,j} \sim N(\mu_j, \sigma_{obs}^2)\)</span></p>
<p><span class="math notranslate nohighlight">\(\sigma_{obs} \sim Exp(1)\)</span></p>
<p><span class="math notranslate nohighlight">\(\mu_j \sim N(\theta, \sigma^2)\)</span></p>
<p><span class="math notranslate nohighlight">\(\sigma \sim Exp(1)\)</span></p>
<p><span class="math notranslate nohighlight">\(\theta \sim N(0,1)\)</span></p>
<p>You can see the mean <span class="math notranslate nohighlight">\(\mu_j\)</span> for the <span class="math notranslate nohighlight">\(j\)</span>th cluster is drawn from the same distribution. In the case there are linear predictors, <span class="math notranslate nohighlight">\(\mu_j\)</span> can be of the form <span class="math notranslate nohighlight">\(\alpha_j + \beta_j X_{i,j}\)</span>. The model is multilevel so long as the data varies at more than one level, in this case for observation <span class="math notranslate nohighlight">\(i\)</span> and cluster <span class="math notranslate nohighlight">\(j\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>

<span class="n">theta_true</span> <span class="o">=</span> <span class="mf">0.6</span>
<span class="n">sigma_true</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">clusters</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">mu_true</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">theta_true</span><span class="p">,</span> <span class="n">sigma_true</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">clusters</span><span class="p">)</span>
<span class="n">sigma_obs_true</span> <span class="o">=</span> <span class="mf">0.3</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mu_true</span><span class="p">,</span> <span class="n">sigma_obs_true</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">clusters</span><span class="p">),</span> <span class="n">size</span> <span class="o">=</span> <span class="mi">30</span><span class="p">)</span><span class="c1">#.flatten()</span>

<span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">multilevel_reg</span><span class="p">:</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;theta&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Exponential</span><span class="p">(</span><span class="s2">&quot;sigma&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;mu&quot;</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">clusters</span><span class="p">)</span>
    <span class="n">sigma_obs</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Exponential</span><span class="p">(</span><span class="s2">&quot;sigma_obs&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">y_obs</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">MvNormal</span><span class="p">(</span><span class="s2">&quot;y&quot;</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma_obs</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">clusters</span><span class="p">),</span> <span class="n">observed</span><span class="o">=</span><span class="n">Y</span><span class="p">)</span>
    <span class="n">trace_multilevel</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">2000</span><span class="p">,</span> <span class="n">tune</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">target_accept</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s1">&#39;adapt_diag&#39;</span><span class="p">)</span>
<span class="n">az</span><span class="o">.</span><span class="n">summary</span><span class="p">(</span><span class="n">trace_multilevel</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Auto-assigning NUTS sampler...
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Initializing NUTS using adapt_diag...
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Multiprocess sampling (4 chains in 4 jobs)
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>NUTS: [theta, sigma, mu, sigma_obs]
</pre></div>
</div>
<div class="output text_html">
    <div>
        <style>
            /* Turns off some styling */
            progress {
                /* gets rid of default border in Firefox and Opera. */
                border: none;
                /* Needs to be in here for Safari polyfill so background images work as expected. */
                background-size: auto;
            }
            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
                background: #F44336;
            }
        </style>
      <progress value='12000' class='' max='12000' style='width:300px; height:20px; vertical-align: middle;'></progress>
      100.00% [12000/12000 00:34<00:00 Sampling 4 chains, 0 divergences]
    </div>
    </div><div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Sampling 4 chains for 1_000 tune and 2_000 draw iterations (4_000 + 8_000 draws total) took 52 seconds.
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>mean</th>
      <th>sd</th>
      <th>hdi_3%</th>
      <th>hdi_97%</th>
      <th>mcse_mean</th>
      <th>mcse_sd</th>
      <th>ess_bulk</th>
      <th>ess_tail</th>
      <th>r_hat</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>theta</th>
      <td>0.168</td>
      <td>0.446</td>
      <td>-0.616</td>
      <td>1.055</td>
      <td>0.005</td>
      <td>0.005</td>
      <td>9672.0</td>
      <td>5235.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>mu[0]</th>
      <td>-0.415</td>
      <td>0.055</td>
      <td>-0.515</td>
      <td>-0.307</td>
      <td>0.001</td>
      <td>0.000</td>
      <td>12281.0</td>
      <td>5860.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>mu[1]</th>
      <td>1.540</td>
      <td>0.055</td>
      <td>1.438</td>
      <td>1.644</td>
      <td>0.001</td>
      <td>0.000</td>
      <td>11703.0</td>
      <td>5293.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>mu[2]</th>
      <td>0.928</td>
      <td>0.057</td>
      <td>0.821</td>
      <td>1.030</td>
      <td>0.001</td>
      <td>0.000</td>
      <td>10977.0</td>
      <td>5882.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>mu[3]</th>
      <td>-0.967</td>
      <td>0.055</td>
      <td>-1.068</td>
      <td>-0.863</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>12344.0</td>
      <td>6196.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>mu[4]</th>
      <td>0.020</td>
      <td>0.057</td>
      <td>-0.086</td>
      <td>0.125</td>
      <td>0.001</td>
      <td>0.001</td>
      <td>12345.0</td>
      <td>6264.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>mu[5]</th>
      <td>2.198</td>
      <td>0.057</td>
      <td>2.092</td>
      <td>2.307</td>
      <td>0.001</td>
      <td>0.000</td>
      <td>12004.0</td>
      <td>5806.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>mu[6]</th>
      <td>-1.751</td>
      <td>0.056</td>
      <td>-1.854</td>
      <td>-1.645</td>
      <td>0.001</td>
      <td>0.000</td>
      <td>12051.0</td>
      <td>6375.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>mu[7]</th>
      <td>0.146</td>
      <td>0.055</td>
      <td>0.042</td>
      <td>0.249</td>
      <td>0.001</td>
      <td>0.000</td>
      <td>11906.0</td>
      <td>5762.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>sigma</th>
      <td>1.376</td>
      <td>0.372</td>
      <td>0.775</td>
      <td>2.046</td>
      <td>0.004</td>
      <td>0.003</td>
      <td>9915.0</td>
      <td>6255.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>sigma_obs</th>
      <td>0.093</td>
      <td>0.009</td>
      <td>0.077</td>
      <td>0.109</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>10861.0</td>
      <td>5982.0</td>
      <td>1.0</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>There are many statistical tools that can help us choose between different linear models. In the case of classical statistics, a number of procedures, such as F-tests for overall significance, likelihood ratio tests and information criteria for goodness of fit, and cross-validation procedures for predictive ability.</p>
<p>It turns out there are Bayesian methods for model selection as well. Due to the fact that the outcome of Bayesian inference is the obtaining of the posterior distribution for each of the model parameters, which summarize all of the parameter uncertainty. This makes the usual t-tests for parameter significance reduntant. The Bayesian credibility intervals have a similar (and simpler) interpretation to that of the frequentist confidence intervals, as they rely on uncertainty regarding the parameters, and not just the sample.</p>
<p>The Bayesian equivalent of the likelihood ratio tests are the so-called Bayes factors. They account for not just the sample, via likelihood, but also the prior distributions. As a consequence of this, Bayes factors with uniform priors shall yield similar results to likelihood ratio tests, as the uniform distribution is proportional to a constant. The formula for the Bayes factor comparing model <span class="math notranslate nohighlight">\(\theta_1\)</span> versus <span class="math notranslate nohighlight">\(\theta_2\)</span> disposing of observed data <span class="math notranslate nohighlight">\(X\)</span> is</p>
<p><span class="math notranslate nohighlight">\(BF = \frac{P(X|\theta_1)}{P(X|\theta_2)} = \frac{P(\theta_1|X)}{P(\theta_2|X)}\frac{P(\theta_2)}{P(\theta_1)}\)</span>.</p>
<p>You can see that the first expression is similar to that of a likelihood ratio. The Bayes factor is also the link between prior and posterior odds for model <span class="math notranslate nohighlight">\(\theta_1\)</span> versus <span class="math notranslate nohighlight">\(\theta_2\)</span>.</p>
<p>With the advance of Bayesian computational methods, calculating information criteria in a fully Bayesian way has also become more feasible. The Watanabe-Akaike information criterion (WAIC) relies on averaging over the whole posterior distribution, and not just posterior moments or maximum likelihood estimates. Cross-validation methods also become more computationally feasible.</p>
<p>A number of graphical procedures have also been proposed. Since, in Bayesian inference, we dispose of the whole statistical model, we can use it to generate new data from the posterior predictive distribution, and compare it to our observed dataset. If the model is a good fit for the observed data, it should be able to generate more data that looks like it. This is the conceptual foundation of the procedure known as posterior predictive checking.</p>
<p>We will explore these procedures later on.</p>
</section>
<section id="additional-references-for-this-section">
<h2>Additional references for this section<a class="headerlink" href="#additional-references-for-this-section" title="Permalink to this heading">#</a></h2>
<p>Schervish, M. J., DeGroot, M. H. (2013). Probability and Statistics. Chapter 11 is an introduction of linear regression from a theoretical statistics perspective, also containing Bayesian interpretations of the linear regression model.</p>
<p>Thomas J. Sargent &amp; John Stachurski. Linear Regression in Python: <a class="reference external" href="https://python.quantecon.org/ols.html">https://python.quantecon.org/ols.html</a>. A great introduction to linear regression in Python, with focus on parameter interpretations and visualization.</p>
<p>McElreath, R. (2020). Statistical Rethinking: A Bayesian Course with Examples in R and Stan. Chapter 4 introduces linear models, Chapter 7 contains model comparison techniques, Chapter 11 introduces GLMs, and Chapters 13 and 14 introduce multilevel modeling.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="CH2.1_cross_section_factors.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">The cross-section of stock returns</p>
      </div>
    </a>
    <a class="right-next"
       href="CH2.3_hierarchical_bayesian_capm.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Bayesian techniques for factor models</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#flat-versus-regularizing-priors">Flat versus regularizing priors</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#generalized-linear-models-glm">Generalized linear models (GLM)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multilevel-modelling">Multilevel modelling</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#additional-references-for-this-section">Additional references for this section</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Thomas Martins
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
<div class="extra_footer">
  This book is available under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International license.
</div>
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>